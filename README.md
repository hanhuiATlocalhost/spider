# spider
基于Scrapoy的分布式网络爬虫系统
   西安工业大学大学生创新创业训练计划项目中期报告
项目编号：201710702015               填写时间： 2018年   09月   10日
项目名称	基于Scrapoy的分布式网络爬虫系统的设计与实现	项目类别	创新创业
负责人	冯树荣	成员	韩辉，韩纪，白永笑，曹伊乐
一、项目进展情况
0．背景调研
1. 概要设计
2. 详细设计
3. 开发
4. 测试
5. 上线
背景调研
技术调研
1、	c++ + linux
学习在linux环境下的c++开发，并学习linux下的程序开发设计，学习linux系统上的各种工具和函数/函数库集，并学习软件开发者工具，如gcc，vim来加快程序开发的进度。
2、	scrapy + python
Scrapy环境搭建并学习python，将scrapy框架使用起来。
Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架。 其可以应用在数据挖掘，信息处理或存储历史数据等一系列的程序中。
其最初是为了页面抓取 (更确切来说, 网络抓取 )所设计的， 也可以应用在获取API所返回的数据(例如 Amazon Associates Web Services ) 或者通用的网络爬虫。Scrapy用途广泛，可以用于数据挖掘、监测和自动化测试。
Scrapy 使用了 Twisted异步网络库来处理网络通讯。整体架构大致如下
 
Scrapy主要包括了以下组件：
•	引擎(Scrapy)
用来处理整个系统的数据流处理, 触发事务(框架核心)
•	调度器(Scheduler)
用来接受引擎发过来的请求, 压入队列中, 并在引擎再次请求的时候返回. 可以想像成一个URL（抓取网页的网址或者说是链接）的优先队列, 由它来决定下一个要抓取的网址是什么, 同时去除重复的网址
•	下载器(Downloader)
用于下载网页内容, 并将网页内容返回给蜘蛛(Scrapy下载器是建立在twisted这个高效的异步模型上的)
•	爬虫(Spiders)
爬虫是主要干活的, 用于从特定的网页中提取自己需要的信息, 即所谓的实体(Item)。用户也可以从中提取出链接,让Scrapy继续抓取下一个页面
•	项目管道(Pipeline)
负责处理爬虫从网页中抽取的实体，主要的功能是持久化实体、验证实体的有效性、清除不需要的信息。当页面被爬虫解析后，将被发送到项目管道，并经过几个特定的次序处理数据。
•	下载器中间件(Downloader Middlewares)
位于Scrapy引擎和下载器之间的框架，主要是处理Scrapy引擎与下载器之间的请求及响应。
•	爬虫中间件(Spider Middlewares)
介于Scrapy引擎和爬虫之间的框架，主要工作是处理蜘蛛的响应输入和请求输出。
•	调度中间件(Scheduler Middewares)
介于Scrapy引擎和调度之间的中间件，从Scrapy引擎发送到调度的请求和响应。
3、	libevent
ibevent是一个轻量级的基于事件驱动的高性能的开源网络库，并且支持多个平台，对多个平台的I/O复用技术进行了封装，当我们编译库的代码时，编译的脚本将会根据OS支持的处理事件机制，来编译相应的代码，从而在libevent接口上保持一致。在当前的服务器上，面对的主要问题就是要能处理大量的连接。
学习libevent底层的实现，即I/O复用技术的实现，epoll，poll，select。
需求调研
1、	什么是爬虫
是一个软件机器人，是可控的，可以可以从互联网上抓取我们所需的资源。爬虫是搜索引擎后台的第一个子系统，数据入口之一。
2、	爬虫能做什么
搜索引擎的基础应用
抓取大数据的一种手段
网页下载器
网店秒杀
3、自动抓取网络资源的软件。
资源是什么？  网页、图片、音乐、视频等
自动化是什么样子？一旦运行就不需要更多的干预。
概要设计
如何思考并完成设计？
从顶层开始思考并设计，避免过早的陷入细节。
 
系统最粗浅的数据流

设计处理流程：
1、得到爬取种子（URL）
2、根据爬取种子下载资源（页面）
3、解析页面，提取更多的URL
4、对页面做持久化操作
5、根据提取的URL再进行下载操作
6、重复第2步到第5步

系统设计
 
系统结构图
详细设计
控制器模块

控制器由三个模块组成：
1、	配置文件处理模块：从配置文件中读取配置项，提供配置项的提取接口
2、	URL维护模块：负责维护URL库，提供如下功能
a)	输入新的URL
b)	输出一个未被抓取的URL
c)	负责维护URL的抓取状态
3、	任务调度模块
a)	负责协调控制器的流程
b)	负责调用其他系统模块完成工作
4、	维护URL列表数据结构
 
URL维护模块

页面抓取的处理流程：
1、	得到一个新的URL
2、	URL进入抓取队列等待抓取
3、	从队列中得到一个URL，把其分配给一个下载器的实例
4、	得到下载器的处理状态（URL处理状态需要被改写，得到当前URL深度，得到当前资源类型假如下载成功）
5、	得到当前页面中存在的下一级URL列表
 
URL维护模块
URL维护模块操作（对外接口）：
1、添加新URL
2、使URL进入抓取队列
3、从抓取队列中移除一个URL
4、修改URL库中某一个URL的值
5、添加新URL的列表
日志工具
日志输出等级设计（5个等级）
0调试[debug]:仅仅用于调试
1普通信息[info]：可以让使用者了解的一些信息
2警告信息[warn]：意味着程序中出现了错误，但是并不严重
4错误信息[error]：意味着程序中发生了严重错误，根据实际情况可选择使程序继续运行或使程序终止
5 程序崩溃[crash]：程序无法继续运行了。
插件框架设计
设计：
1、	动态载入.so文件
2、	维护.so文件中的接口函数
设计原则：
1、	一个功能一个模块
2、	可以自我维护
3、	设计入口函数指针原型
int(*handle)(void*);
设计初始化函数指针原型
int(*init)(Module*);
模块管理器设计：
1载入模块的操作
Int Load (char* path, char*name);
Module* getModule(char* name);
载入模块操作的处理流程：
1通过路径找到模块文件(.so)
2调用dlopen打开动态库（.so）
3使用动态库
4关闭动态库文件
 
开发
按照概要设计和详细设计进行代码翻译，并使用git对项目进行管理。
测试
分为单元测试和整体测试，单元测试保证功能和逻辑正确；整体测试为后序调优，查找性能瓶颈做铺垫。
上线
还未进行设计开发，最终目的是用shell脚本将软件做成系统服务，用脚本启动程序。
二、已取得的成果
项目初步设计完成并产出文档，完成基本开发。代码时间跨度较大，模块之间调度存在bug，正在排查，还需要时间进行调试并fix。
三、经费开支情况
购买办公用品花费50元；
购买打印纸等耗材花费150元；
购买相关学习书籍花费400元。
小计：600元

四、存在的问题和困难 
1.需要code review，对自己实现的线程池和内存池，代码健壮性需要检查。
2.对数据库的知识不够充足，数据库底层原理一无所知，仅仅会调用sql的API
3.希望使用目前业界较为前沿的技术，解决集群问题，如kafak，redis，hadoop，hbase，flink。
4.数据库选型，由于对数据库的知识不够充实，目前暂且使用mongoDB，但数据库的I/O在上千的QPS中会成为瓶颈，后序希望能使用hbase或类似hadoop中hive的分布式数据库。

五、建议和要求
1.分布式爬虫中分布式的测试极为困难，希望得到学校帮助，使用5台以上的机器构建集群进行功能测试和性能测试，并进行性能压测和对性能的改进。
六、指导教师意见



                                                 签字
七、学院意见



部门盖章（主管领导签名）：      年   月   日
八、学校意见


领导签名（盖章）：      年   月   日

